#!/usr/bin/perl
use strict;
use warnings;
use Getopt::Long qw(:config no_getopt_compat);
use List::Util 'max';
use List::MoreUtils qw(any all);
use FindBin '$Bin';
use lib "$Bin/lib";
use Asciilog::Util 'get_unbuffered_line';

use feature qw(say state);


my $usage =  <<EOF;
$0 [--has c0,c1,...] [--has c2] [--print|--pick|-p c3,+c4,sum=c5+c6,rel(c7)] [match_expr] [match_expr] ...

    Other available options:
      --eval expr
      --noskipempty
      --skipcomments
      --dumpexprs
      --perl
      --unbuffered

    --perl processes the data with perl, not awk. Awk is faster and is thus the
    --default. The --matches and --eval expressions are either in awk or perl,
    --depending on --perl

    Columns selected by asking for them with -p, either with multiple -p options
    or with comma-separated lists. Column specs can be regexen. Precede col name
    with '+' to also add that column to --has. A few functions exist to apply
    simple pre-processing transformations to the columns.

    Rows selected with --has and/or arbitrary match expressions.

    --unbuffered flushes each line after each print. Useful for streaming data.

    For more information invoke $0 --help

EOF

if(! @ARGV)
{
    die $usage;
}

# by default we do skip empty records
my %options = (skipempty => 1);
GetOptions(\%options,
           "has=s@",
           "pick|print|p=s@",
           "eval=s",
           "skipempty!",
           "skipcomments!",
           "dumpexprs!",
           "perl",
           "unbuffered",
           "help") or die($usage);
$options{has}  //= [];
$options{pick} //= [];

# anything remaining on the commandline are 'matches' expressions
$options{matches} = \@ARGV;

if( defined $options{eval} )
{
    $options{skipcomments} = 1;
}

if( defined $options{eval} && @{$options{pick}} )
{
    say STDERR "--eval is given, so no column selectors should be given also";
    die $usage;
}

if( defined $options{help} )
{
    print $usage;
    exit 0;
}

# parse the , in $options{has} and $options{pick}
for my $listkey (qw(has pick))
{
    @{$options{$listkey}} = map split(/,/, $_), @{$options{$listkey}};
}

# any requested columns preceded with '+' go into --has. And I strip out the '+'
for my $ipick(0..$#{$options{pick}})
{
    # handle extra column syntax here
    if( ${$options{pick}}[$ipick] =~ /^\+(.+)/ )
    {
        ${$options{pick}}[$ipick] = $1;
        push @{$options{has}}, ${$options{pick}}[$ipick];
    }
}

my @picked_exprs_named  = @{$options{pick}};
my @must_have_col_names = @{$options{has}};
my @must_have_col_indices_input;

# if no columns requested, just print everything
if( !@picked_exprs_named && !@must_have_col_names && !@{$options{matches}} && !defined $options{eval} )
{
    while(<STDIN>)
    {
        print;
        flush STDOUT if $options{unbuffered};
    }
}

my @colnames_output;

# input column-name to index map. If I have more than one of the same column,
# this maps to undef. Not an error unless we try to use the columns
my %colindices_input;

my $colidx_needed_max = -1;



# awk or perl strings representing stuff to output. These are either simple
# column references (such as $1), or more complex expressions
my @langspecific_output_fields;

# How many rel(),diff() calls we have. I generate code based on this
my $Nrel  = 0;
my $Ndiff = 0;


# Loop searching for the legend.
#
# Here instead of using while(<STDIN>) we read one byte at a time. This means
# that as far as the OS is concerned we never read() past our line. And when we
# exec() to awk, all the data is available. This is inefficient, but we only use
# this function to read up to the legend, which is fine.
#
# Note that perl tries to make while(<STDIN>) work by doing an lseek() before we
# exec(), but if we're reading a pipe, this can't work
while(defined ($_ = get_unbuffered_line(*STDIN)))
{
    # I pass through (don't treat as a legend) ## comments and #! shebang
    if(/^#[#!]/p)
    {
        unless($options{skipcomments})
        {
            print;
            flush STDOUT if $options{unbuffered};
        }
        next;
    }

    if( /^#/p )
    {
        chomp;

        # we got a legend line
        my @cols_all_legend_input = split ' ', ${^POSTMATCH}; # split the field names (sans the #)
        foreach my $idx (0..$#cols_all_legend_input)
        {
            if( exists $colindices_input{$cols_all_legend_input[$idx]} )
            {
                $colindices_input{$cols_all_legend_input[$idx]} = undef;
            }
            else
            {
                $colindices_input{$cols_all_legend_input[$idx]} = $idx;
            }
        }

        # If we weren't asked for particular columns, take them all. This isn't
        # a no-op because we can have --has
        @picked_exprs_named = @cols_all_legend_input unless @picked_exprs_named;

      COLUMN:
        foreach my $picked_expr_named (@picked_exprs_named)
        {
            my $accept = sub
            {
                my ($exprs, $name) = @_;

                if( defined $name )
                {
                    if( @$exprs != 1 )
                    {
                        die "A named expression can only refer to ONE column";
                    }
                    push @colnames_output, $name;
                }
                else
                {
                    push @colnames_output, @$exprs;
                }

                my @output_fields;
                for my $expr(@$exprs)
                {
                    $Nrel  = subst_rel (\$expr);
                    $Ndiff = subst_diff(\$expr);

                    my ($output_field, $colidx_needed_max_here) =
                      expr_subst_col_names($options{perl} ? 'perl' : 'awk',
                                           $expr);
                    if ( $colidx_needed_max_here > $colidx_needed_max )
                    {
                        $colidx_needed_max = $colidx_needed_max_here;
                    }
                    push @output_fields, $output_field;
                }
                push @langspecific_output_fields, @output_fields;
            };







            my ($name, $picked_expr) = $picked_expr_named =~ /(.*?)=(.*)/;
            if( !defined $picked_expr )
            {
                $picked_expr = $picked_expr_named;
            }


            # do I have an exact column match?
            if( defined $colindices_input{$picked_expr} )
            {
                # We have exactly ONE matched column
                $accept->( [$picked_expr], $name);
                next;
            }
            if( exists $colindices_input{$picked_expr} )
            {
                die "Found more than one column that string-matched '$picked_expr' exactly";
            }

            # No exact column match. If this is a named expression, I pass it on
            # to awk/perl
            if( defined $name )
            {
                $accept->([$picked_expr], $name);
                next;
            }

            # No exact matches were found, and not a named expression. Let me
            # try a regex
            my $picked_expr_re;
            eval { $picked_expr_re = qr/$picked_expr/; };
            if( !$@ )
            {
                # compiled regex successfully
                my @matching_input_cols = grep /$picked_expr_re/, @cols_all_legend_input;
                if( @matching_input_cols >= 1 )
                {
                    $accept->(\@matching_input_cols);
                    next;
                }
            }

            die "Couldn't find requested column '$picked_expr' in the legend line '$_'";
        }

        # print out the new legend
        unless($options{dumpexprs} || $options{eval})
        {
            print "# @colnames_output\n";
            flush STDOUT if $options{unbuffered};
        }


        if( @must_have_col_names )
        {
            foreach my $col (@must_have_col_names)
            {
                if( !defined $colindices_input{$col})
                {
                    die "I don't have column '$col'";
                }
                push @must_have_col_indices_input, $colindices_input{$col};
            }
        }
        last;
    }

    die "Got data line before a legend";
}

if(!%colindices_input)
{
    die "No legend received. Is the input file empty?";
}






# At this point I'm done dealing with the legend, and it's time to read in and
# process the data. I can keep going in perl, or I can generate an awk program,
# and let awk do this work. The reason: awk (mawk especialy) runs much faster.
# Both paths should produce the exact same output, and the test suite makes sure
# this is the case

if( !$options{perl} )
{
    my $awkprogram = makeAwkProgram();
    if( $options{dumpexprs} )
    {
        say $awkprogram;
        exit;
    }
    if($options{unbuffered})
    {
        exec 'mawk', '-Winteractive', $awkprogram;
    }
    else
    {
        exec 'mawk', $awkprogram;
    }

    exit; # dummy. We never get here
}

sub expr_subst_col_names
{
    # I take in a string with awk/perl code, and replace field references to
    # column references that the awk/perl program will understand. To minimize
    # the risk of ambiguous matches, I try to match longer strings first
    my ($language, $out) = @_;

    my $colidx_needed_max_here = -1;

    for my $key(reverse sort {length($a) <=> length($b)} keys %colindices_input)
    {
        if( $language eq 'perl' )
        {
            my $post = $key =~ /\w$/ ? '\b' : '';
            my $re = qr/\Q$key\E$post/;
            my $found = $out =~ s/$re/\$f->[$colindices_input{$key}]/g;

            if($found && $colindices_input{$key} > $colidx_needed_max_here)
            {
                $colidx_needed_max_here = $colindices_input{$key};
            }
        }
        elsif( $language eq 'awk' )
        {
            # column index that awk knows about
            my $colidx = $colindices_input{$key} + 1;
            my $re = qr/\b$key\b/;
            my $found = $out =~ s/$re/\$$colidx/g;

            if($found && $colindices_input{$key} > $colidx_needed_max_here)
            {
                $colidx_needed_max_here = $colindices_input{$key};
            }
        }
        else
        {
            die "Unknown language '$language";
        }
    }
    return ($out, $colidx_needed_max_here);
}

sub subst_rel
{
    my ($expr) = @_;

    state $relidx = 0;

    while( $$expr =~ /\brel\s*\(/ )
    {
        $$expr =~ s/\brel(\s*\()/rel$relidx$1/;
        $relidx++;
    }
    return $relidx;
}
sub subst_diff
{
    my ($expr) = @_;

    state $diffidx = 0;

    while( $$expr =~ /\bdiff\s*\(/ )
    {
        $$expr =~ s/\bdiff(\s*\()/diff$diffidx$1/;
        $diffidx++;
    }
    return $diffidx;
}


my $must_match_expr =
  join ' && ',
  map { my ($outexpr) = expr_subst_col_names( 'perl', $_); $outexpr; }
  @{$options{matches}};
$must_match_expr = 1 if !defined $must_match_expr || '' eq $must_match_expr;

my $evalstr = 'sub matches { my ($f) = @_; return ' . $must_match_expr . '; }';

if ( $options{eval} )
{
    my ($expr) = expr_subst_col_names( 'perl', $options{eval} );
    $evalstr .= "\n" . 'sub evalexpr { my ($f) = @_; ' . $expr . ' ; }';
}

$evalstr .=
  "\n" . 'sub compute_output_fields { my ($f) = @_; return [' . join(',', @langspecific_output_fields) . ']; }';






if( $options{dumpexprs} )
{
    say "Expressions to evaluate:\n\n$evalstr";
    exit;
}

eval $evalstr;
if( $@ )
{
    die "Error evaluating expression '$evalstr':\n$@";
}


# I'm defining the rel()/diff() functions. These should be global, so if I do
# this inside a for(){}, the functions end up local to that for(). I thus have
# an ugly manual loop
my $i = 0;
EVAL_REL_FUNC:
eval "sub rel$i" . '{ my ($x) = @_; state $state=undef; if(!defined $state) { $state=$x; } return $x - $state; } ';
if($i++ < $Nrel) { goto EVAL_REL_FUNC; }

$i = 0;
EVAL_DIFF_FUNC:
eval "sub diff$i" . '{ my ($x) = @_; state $inited=0; state $state=0; my $retval = ($x - $state)*$inited; $state = $x; $inited=1; return $retval; } ';
if($i++ < $Ndiff) { goto EVAL_DIFF_FUNC; }


RECORD:
while(<STDIN>)
{
    # Data loop. Each statement here is analogous to the awk program generated
    # by makeAwkProgram();

    # skip comments
    if(/^#/)
    {
        unless($options{skipcomments})
        {
            print;
            flush STDOUT if $options{unbuffered};
        }
        next;
    }

    chomp;
    my @f = map {q{-} eq $_ ? undef : $_ } split;

    # skip incomplete records. Can happen if a log line at the end of a file was
    # cut off in the middle
    next unless $colidx_needed_max <= $#f;

    # skip records that have empty input columns that must be non-empty
    next if any {!defined $f[$_]} @must_have_col_indices_input;

    # skip all records that don't match given expressions
    next unless matches(\@f);

    if( $options{eval} )
    {
        evalexpr(\@f);
    }
    else
    {
        my $fout = compute_output_fields(\@f);

        # skip empty records if we must
        next if $options{skipempty} && all {!defined $_} @$fout;

        say join(' ', map {$_ // '-'} @$fout);
        flush STDOUT if $options{unbuffered};
    }
}










sub makeAwkProgram
{
    # The awk program I generate here is analogous to the logic in the data
    # while() loop above
    my $awkprogram = '';

    for my $i (0..$Nrel-1)
    {
        $awkprogram .= "function rel$i(x) { if(!__inited_rel$i) { __state_rel$i = x; __inited_rel$i = 1; } return x - __state_rel$i; } ";
    }
    for my $i (0..$Ndiff-1)
    {
        $awkprogram .= "function diff$i(x) { retval = (x - __state_diff$i)*__inited_diff$i; __state_diff$i = x; __inited_diff$i = 1; return retval; } ";
    }


    # skip comments
    $awkprogram .=
      '/^#/ { ' . ($options{skipcomments} ? '' : 'print; ') . 'next } ';

    # skip incomplete records. Can happen if a log line at the end of a file
    # was cut off in the middle
    $awkprogram .= (1+$colidx_needed_max) . " > NF { next } ";

    # skip records that have empty input columns that must be non-empty
    if (@must_have_col_indices_input)
    {
        $awkprogram .=
          join(' || ', map { '$'.($_+1). " == \"-\"" } @must_have_col_indices_input);
        $awkprogram .= ' { next } ';
    }

    for my $expr( @{$options{matches}} )
    {
        ($expr) = expr_subst_col_names('awk', $expr);
        $awkprogram .= ' !' . "($expr) { next } ";
    }

    if( $options{eval} )
    {
        my ($expr) = expr_subst_col_names('awk', $options{eval});
        $awkprogram .= "$expr ";
    }
    else
    {
        # skip empty records if we must. I evaluate the fields just one time to
        # not affect the state inside rel() and diff()
        if (!$options{skipempty})
        {
            # print selected fields
            $awkprogram .=
              '{ print ' . (join(',', @langspecific_output_fields)) . ' } ';
        }
        else
        {
            # I evaluate and cache all the fields
            $awkprogram .=
              '{ ' .
              join(' ',
                   map { "__f$_ = $langspecific_output_fields[$_]; " }
                   0..$#langspecific_output_fields) .

                     # Then I do skipempty. Important to do this after evaluating ALL the
                     # fields to tick all the rel(), diff() state
                     "if(" . join( ' && ', map { "__f$_  == \"-\""} 0..$#langspecific_output_fields ) .
                     ") { next }; " .

                     # And THEN I print everything
                     'print ' . (join(',', map {"__f$_"} 0..$#langspecific_output_fields)) . '} ';
        }
    }

    return $awkprogram;
}

__END__

=head1 NOTICE

This is undergoing a rework. Don't read (and believe) the documentation just
yet.


=head1 NAME

asciilog-filter - filters ascii logs to select particular rows, fields

=head1 SYNOPSIS

    # Read log data, filter out the timestamps, and post-process some of them
    $ < /tmp/log
      asciilog-filter '.*time' 'us2s(time)' 'rel(time)' 'rel(us2s(.*time))'

# this is bugged. also get rid of the ()
    # image_time time us2s(time) rel(time) rel(us2s(image_time)) rel(us2s(time))
    1434662133270338 1434662131279978 1434662131.27998 0 0 0
    1434662133270338 1434662131289978 1434662131.28998 10000 0 0.00999999046325684
    1434662133270338 1434662131299978 1434662131.29998 20000 0 0.0199999809265137
    1434662133270338 1434662131309978 1434662131.30998 30000 0 0.0299999713897705
    ...

    # Run the robot, write out telemetry to a log, filter out the position, and
    # make a realtime plot of its motion
    $ run_robot |
      tee /tmp/log |
      asciilog-filter --unbuffered x y |
      feedgnuplot --domain --lines --stream

    [ plot pops up, and updates itself as the robot moves ]


=head1 DESCRIPTION

This tool reads in an ASCII data stream, and allows easy filtering to select
particular data from this stream. Many common post-processing operations are
available, as is general interpretation of data.

This is a UNIX-style tool, so the input/output of this tool is strictly
STDIN/STDOUT. Furthermore, in its most common form this tool is a filter, so the
format of the output is I<exactly> the same as the format of the input. The
exception to this is when using C<--eval>, in which the output is dependent on
whatever expression we're evaluating.

This tool is convenient both to process stored data, or to process live data
that can then be plotted to produce realtime telemetry.

This tool takes a list of fields on the commandline. These are the only fields
that are selected for output. The requested field names are compared with the
fields listed in the legend of the data. If an exact match is found, we select
that column. Otherwise we run a regex search, and take all matching columns.

The user often wants to apply unit conversions to data, or to look at the data
relative to the initial point, or to differentiate the input. These filters can
be easily applied by this tool.

This tool has two major modes of operation: filtering and evaluating.

The normal mode of operation is a filtering one. Here we read in an asciilog,
select some rows, columns, maybe apply some simple filter functions, and output
this data as a new asciilog. A new legend is written, and comments are
preserved.

If we want to do something more complicated, we can use the I<evaluating> mode
of operation. Here we pass C<--eval expression>. For each row, we evaluate the
expression (in awk, or in perl if C<--perl> is given). This evaluation may or
may not print anything. Comments are ignored. Any columns should be referred-to
by name (not as a regex or a transform). This mode ignores any column selectors,
so none should be given.

=head2 Input/output data format

The input/output data is simply an ASCII table of values. Any lines beginning
with C<##> are treated as comments, and are passed through. The first line that
begins with C<#> but not C<##> is a I<legend> line. After the C<#>, follow
whitespace-separated ASCII field names. Each subsequent line is
whitespace-separated values matching this legend. For instance, this is a valid
data file:

    ## log version: 3 ins_type: RAW_LOG_INS_440 camera_type: Unknown camera_type id: 5
    ## camera 0: serial 0,1 cols/rows: 3904 3904 channels: 1 depth: 8
    ## camera 1: serial 2,3 cols/rows: 3904 3904 channels: 1 depth: 8
    ## camera 2: serial 4,0 cols/rows: 3904 3904 channels: 1 depth: 8
    ## camera 3: serial 0,0 cols/rows: 0 0 channels: 0 depth: 0
    # x_rate y_rate z_rate
    -0.016107 0.004362 0.005369
    -0.017449 0.006711 0.006711
    -0.018456 0.014093 0.006711
    -0.017449 0.018791 0.006376

This is the format for both the input and the output. This tool makes sure to
update the legend to reflect which columns have been selected.

A string C<-> is used to indicate that a record does not have a value for this
field. Other records (lines) may have such a value.

=head2 Basic filtering

To select specific columns from a file, pass their names (or regexen) on the
commandline. For instance to pull out columns called C<lat>, C<lon>, and any
column whose name contains the string C<feature_>, do

 asciilog-filter lat lon feature_

We look for exact column name matches, and if none are found, we use a regex. So
the above would work if we had columns C<feature_width> and C<feature_height>,
but would fail if we also had C<feature_>, since in this case only this last
column would be found. We can explicitly pass a regex to deal with this:

 asciilog-filter lat lon 'feature_.*'

To select specific rows, we can use C<--has col> to get only those rows that
have a non-empty value in column C<col>. C<--matches> is similar, but evaluates
arebitrary expressions. For instance C<--matches 'size E<gt> 10'> would select
only those rows whose C<size> column contains a value > 10. Note that
C<--matches> can I<only> see data currently in the input asciilog, and can
I<not> apply L<transform expressions|Transforms>. So if you want C<--matches
rel(x)E<gt>5> then you must first invoke C<asciilog-filter> to produce this
column. I.e. this doesn't work:

 <data asciilog-filter --matches 'rel(x)>5'

But this /does/ work:

 <data asciilog-filter . 'rel(x)' | asciilog-filter --matches 'rel(x)>5'

If we want to select a column I<and> pick only rows that have a value in this
column, a shorthand syntax exists:

 asciilog-filter +col

is equivalent to

 asciilog-filter --has col col

=head2 Transforms

We can post-process our data with some simple transforms. To apply a transform
=f= and then a transform =g= to column =x=, pass in =g(f(x))=, as expected. Note
that the resulting legend contains fields with =()= in their names, but these
remain filter-able with further =asciilog-filter= invocations, and C<--matches>
and C<--eval> remain functional. The only weirdness is that if we're using perl
you need to say C<--matches $rel(x)E<gt>5> instead of C<--matches rel($x)E<gt>5>.

The transforms currently available are

=over

=item C<rel_n>, C<rel_e>

Can be applied to latitude and longitude (in degrees) respectively to report
coordinates in a local coordinates system aligned with North and East relative
to the first latlon, in meters. If using these, you I<must> have one of each and
C<rel_n> I<must> come first. Generally you'll filter thusly:

    'rel_n(lat)' 'rel_e(lon)'

=back

=head2 Backend choice

By default, the parsing of arguments and the legend happens in perl, which then
constructs a simple awk script, and invokes C<awk> to actually read the data and
to process it. This is done because awk is lighter weight and runs faster, which
is important because our data sets could be quite large. This is especially true
of C<mawk>, which is noticeably more snappy than C<gawk>. We don't need the
extra features of C<gawk>, so C<mawk> is preferred here. If for whatever reason
we want to do everything with perl, this can be requested with the C<--perl>
option.

=head1 ARGUMENTS

=head2 --has a,b,c,...

Used to select particular records (rows) in a data file. A I<null> value in a
column is designated with a single C<->. This means that this particular field
value does not exist in this record. If we want to select only records that
I<do> have a column named C<x>, we can pass C<--has x>. To select records that
have data for I<all> of a set of columns, the C<--has> option can be repeated,
or these multiple columns can be given in a whitespace-less comma-separated
list. For instance if we want only records that have data in I<both> columns
C<x> I<and> C<y> we can pass in C<--has x,y> or C<--has x --has y>. If we want
to combine multiple columns in an I<or> (select rows that have data in I<any> of
a given set of columns), use C<--matches> as documented below.

If we want to select a column I<and> pick only rows that have a value in this
column, a shorthand syntax exists:

 asciilog-filter --has col col

is equivalent to

 asciilog-filter +col

=head2 --matches expr

Used to select particular records (rows) in a data file. The argument is an
expression that is evaluated for each row. If it evaluates to true, that row is
output. This expression is passed directly to the perl or awk backend, so be
aware of which one you are using. This feature is slow in perl, so awk is
strongly recommended here. To refer to a field named C<xxx> in the expression,
say C<xxx> in awk and C<$xxx> in perl. There's a small caveat here: C<xxx> is a
field-name, even if it contains non-alphanumerics, so if you want to pull out
only those records where C<rel(x) E<gt> 5>, then in perl you say this:

 <data asciilog-filter . 'rel(x)' | asciilog-filter --perl --matches '$rel(x) > 5'

Note the C<$rel(x)> and /not/ C<rel($x)>.

Multiple C<--matches> options can be given. The record is output only if I<all>
the expressions are true.

If a C<--matches> expression begins with C<@>, then the text following the C<@>
is assumed to be a filename from which the expression should be read.

Example: to select all rows that have valid data in column C<a> I<or> column
C<b> I<or> column C<c> you can

 asciilog-filter --matches 'a != "-" || b != "-" || c != "-"'

or

 asciilog-filter --perl --matches 'defined $a || defined $b || defined $c'

=head2 --eval expr

Instead of printing out all matching records, evaluate the expression for each
record. The expression may or may not print anything. The expression is treated
similar to --matches: variables are bound by name, a sigil may be required,
depending on language and so on. No column-selecting arguments should be given.

If a C<--eval> expression begins with C<@>, then the text following the C<@> is
assumed to be a filename from which the expression should be read.

In perl the arbitrary expression fits in like this:

 while(<>) # read each line
 {
   next unless matches; # skip non-matching lines
   eval expression;     # evaluate the arbitrary expression
 }

In awk the expr is a full set of pattern/action statements. So to print the sum
of C<a> and C<b>:

 asciilog-filter --eval '{print a+b}'

To print the sum of some column:

 asciilog-filter --eval '{sum += x} END {print sum}'

=head2 --[no]skipempty

Do [not] skip records where all fields are blank. By default we I<do> skip all
empty records; to include them, pass C<--noskipempty>

=head2 --skipcomments

Don't output non-legend comments

=head2 --perl

Normally we initialize with perl, and use awk to do all the actual work. If we
want to do everything with perl, pass this option. It should produce the same
results, but not as quickly.

=head2 --dumpexprs

Used for debugging. This spits out all the expressions parsed from the
commandline. These come from C<--matches>, C<--eval> and, if using the awk
backend, the generated awk program

=head2 --unbuffered

Flushes each line after each print. This makes sure each line is output as soon
as it is available, which is crucial for realtime output and plotting.

=head1 REPOSITORY

https://github.com/dkogan/asciilog/

=head1 AUTHOR

Dima Kogan C<< <dima@secretsauce.net> >>

=head1 LICENSE AND COPYRIGHT

Copyright 2016-2017 California Institute of Technology
Copyright 2017-2018 Dima Kogan


This library is free software; you can redistribute it and/or modify it under
the terms of the GNU Lesser General Public License as published by the Free
Software Foundation; either version 2.1 of the License, or (at your option) any
later version.

=cut
