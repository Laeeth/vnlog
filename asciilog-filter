#!/usr/bin/perl
use strict;
use warnings;
use Getopt::Long qw(:config no_getopt_compat);
use List::Util 'max';
use List::MoreUtils qw(any all);
use FindBin '$Bin';
use lib "$Bin/lib";
use Asciilog::Util 'get_unbuffered_line';

use feature qw(say state);


my $usage =  <<EOF;
$0 [--has c0,c1,...] [--has c2] --print|--pick|-p c3,c4,+c5,f(c0),g(c1) [match_expr|\@match_expr_file] ...

    Other available options:
      --eval expr|\@file
      --noskipempty
      --skipcomments
      --dumpindices
      --dumpexprs
      --perl
      --unbuffered

    --perl processes the data with perl, not awk. Awk is faster and is thus the
    --default. The --matches and --eval expressions are either in awk or perl,
    --depending on --perl

    Columns selected by asking for them with -p, either with multiple -p options
    or with comma-separated lists. Column specs can be regexen. Precede col name
    with '+' to also add that column to --has. A few functions exist to apply
    simple pre-processing transformations to the columns.

    Rows selected with --has and/or arbitrary match expressions.

    --unbuffered flushes each line after each print. Useful for streaming data.

    For more information invoke $0 --help

EOF

if(! @ARGV)
{
    die $usage;
}

# by default we do skip empty records
my %options = (skipempty => 1);
GetOptions(\%options,
           "has=s@",
           "pick|print|p=s@",
           "eval=s",
           "skipempty!",
           "skipcomments!",
           "dumpindices!",
           "dumpexprs!",
           "perl",
           "unbuffered",
           "help") or die($usage);
$options{has}  //= [];
$options{pick} //= [];

# anything remaining on the commandline are 'matches' expressions
$options{matches} = \@ARGV;

if( defined $options{eval} )
{
    $options{skipcomments} = 1;
}

if( defined $options{eval} && @{$options{pick}} )
{
    say STDERR "--eval is given, so no column selectors should be given also";
    die $usage;
}

if( defined $options{help} )
{
    print $usage;
    exit 0;
}

# parse the , in $options{has} and $options{pick}
for my $listkey (qw(has pick))
{
    @{$options{$listkey}} = map split(/,/, $_), @{$options{$listkey}};
}

# any requested columns preceded with '+' go into --has. And I strip out the '+'
for my $ipick(0..$#{$options{pick}})
{
    if( ${$options{pick}}[$ipick] =~ /^\+(.+)/ )
    {
        ${$options{pick}}[$ipick] = $1;
        push @{$options{has}}, ${$options{pick}}[$ipick];
    }
}

my @cols_want           = @{$options{pick}};
my @must_have_col_names = @{$options{has}};
my @must_have_col_indices_in;

# if no columns requested, just print everything
if( !@cols_want && !@must_have_col_names && !@{$options{matches}} && !defined $options{eval} )
{
    while(<STDIN>)
    {
        print;
        flush STDOUT if $options{unbuffered};
    }
}

my @cols_all_legend;
my @colnames_output;
my %colindices_in;

my @indices_in = ();
my $indices_in_max;


# these are all for transforms: things like rel(a), deg2rad(x) and so on
my @transforms;        # perl functions
my @awk_output_fields; # awk strings representing stuff to print
# awk stanzas for set up any required state. This is for rel() to store the
# initial value and diff() to store the previous value
my ($awk_state_stanzas_before, $awk_state_stanzas_after);

my $saw_rel_n;


# Loop searching for the legend.
#
# Here instead of using while(<STDIN>) we read one byte at a time. This means
# that as far as the OS is concerned we never read() past our line. And when we
# exec() to awk, all the data is available. This is inefficient, but we only use
# this function to read up to the legend, which is fine.
#
# Note that perl tries to make while(<STDIN>) work by doing an lseek() before we
# exec(), but if we're reading a pipe, this can't work
while(defined ($_ = get_unbuffered_line(*STDIN)))
{
    # I pass through (don't treat as a legend) ## comments and #! shebang
    if(/^#[#!]/p)
    {
        unless($options{skipcomments})
        {
            print;
            flush STDOUT if $options{unbuffered};
        }
        next;
    }

    if( /^#/p )
    {
        chomp;

        # we got a legend line
        @cols_all_legend = split ' ', ${^POSTMATCH}; # split the field names (sans the #)
        foreach my $idx (0..$#cols_all_legend)
        {
            $colindices_in{$cols_all_legend[$idx]} = $idx;
        }

        # If we weren't asked for particular columns, take them all. This isn't
        # a no-op because we can have --has
        @cols_want = @cols_all_legend unless @cols_want;

        # grab all the column indices
      COLUMN:
        foreach my $col_want (@cols_want)
        {
            my @funcs;

            my $accept = sub
            {
                my @indices_in_here = @_;

                my $Nindices = @indices_in;

                push @colnames_output, @cols_all_legend[@indices_in_here];
                push @indices_in,      @indices_in_here;

                if( !@funcs )
                {
                    push @awk_output_fields, map { '$'.($_+1) } @indices_in_here;
                }
                else
                {
                    # This loop is important. It is possible to push it to later
                    # by doing this instead:
                    #
                    #   push @transforms, [\@indices_in_here, parse_transform_funcs(@funcs) ];
                    #
                    # but then all of @indices_in_here will get a single
                    # transformation subroutine object, and all of its internal
                    # state will be shared, which is NOT what you want. For
                    # instance if we're doing rel(.*time) or something, then the
                    # initial timestamp would be shared. This is wrong.
                    #
                    # The indices here index the OUTPUT columns list
                    foreach my $idx (0..$#indices_in_here)
                    {
                        my $idx_out = $idx + $Nindices;

                        my @perl_funcs         = parse_transform_funcs(@funcs);
                        my $awk_stanzas_after  = pop @perl_funcs;
                        my $awk_stanzas_before = pop @perl_funcs;
                        my $awk_statement      = pop @perl_funcs;

                        push @transforms,        [$idx_out, @perl_funcs ];

                        my $idx_in        =  $indices_in_here[$idx];
                        $awk_statement    =~ s/___input___/'$'.($idx_in+1)/ge;
                        push @awk_output_fields, $awk_statement;

                        if(defined $awk_stanzas_before)
                        {
                            $awk_stanzas_before      =~ s/___input___/'$'.($idx_in+1)/ge;
                            $awk_state_stanzas_before .= $awk_stanzas_before;
                        }
                        if(defined $awk_stanzas_after)
                        {
                            $awk_stanzas_after      =~ s/___input___/'$'.($idx_in+1)/ge;
                            $awk_state_stanzas_after .= $awk_stanzas_after;
                        }

                        $colnames_output[$idx_out] =
                          join('', map { "$_("} reverse @funcs) .
                          $colnames_output[$idx_out] .
                          ')' x scalar(@funcs);
                    }
                }
            };


            my @indices_in_here;

            # do I have an exact column match?
            @indices_in_here = grep {$col_want eq $cols_all_legend[$_]} 0..$#cols_all_legend;
            if ( @indices_in_here > 1 )
            {
                die "Found more than one column that string-matched '$col_want' exactly";
            }
            if ( @indices_in_here == 1 )
            {
                $accept->(@indices_in_here);
                next;
            }

            # I look for any requested functions. These all look like "f(x)" and
            # may be nested. f(g(h(x))) is allowed. At each function level, I
            # look for exact matching columns. I.e. if I'm asked about
            # "diff(rel(x))" it is possible that I should find a column "x" and
            # then rel() it and then diff() it; it's also possible that I should
            # find a column "rel(x)" and then diff() it
            while($col_want =~ /^           # start
                                ( [^\(]+ )  # Function name. Non-( characters
                                \( (.+) \)  # Function arg
                                $           # End
                               /x)
            {
                unshift @funcs, $1;
                $col_want = $2;

                # do I have an exact column match now?
                @indices_in_here = grep {$col_want eq $cols_all_legend[$_]} 0..$#cols_all_legend;
                if ( @indices_in_here > 1 )
                {
                    die "Found more than one column that string-matched '$col_want' exactly";
                }
                if ( @indices_in_here == 1 )
                {
                    $accept->(@indices_in_here);
                    next COLUMN;
                }

            }

            # I'm done looking for transformations, and no exact matches were
            # found. I know there aren't any more '()' in the string, so I try a
            # regex
            @indices_in_here = grep {$cols_all_legend[$_] =~ qr/$col_want/} 0..$#cols_all_legend;
            if( @indices_in_here >= 1 )
            {
                $accept->(@indices_in_here);
                next;
            }

            die "Couldn't find requested column '$col_want' in the legend line '$_'";
        }

        $indices_in_max = max @indices_in;

        if ( $options{dumpindices} )
        {
            print "@indices_in\n";
            exit;
        }

        # print out the new legend
        unless($options{dumpexprs} || $options{eval})
        {
            print "# @colnames_output\n";
            flush STDOUT if $options{unbuffered};
        }


        if( @must_have_col_names )
        {
            foreach my $col (@must_have_col_names)
            {
                if( !defined $colindices_in{$col})
                {
                    die "I don't have column '$col'";
                }
                push @must_have_col_indices_in, $colindices_in{$col};
            }
        }
        last;
    }

    die "Got data line before a legend";
}

if(!@cols_all_legend)
{
    die "No legend received. Is the input file empty?";
}





# data loop
#
# At this point I'm done dealing with the legend, and it's time to read in and
# process the data. I can keep going in perl, or I can generate an awk program,
# and let awk do this work. The reason: awk (mawk especialy) runs much faster.
# Both paths should produce the exact same output, and the test suite makes sure
# this is the case

if( !$options{perl} )
{
    my $awkprogram = makeAwkProgram();
    if( $options{dumpexprs} )
    {
        say $awkprogram;
        exit;
    }
    if($options{unbuffered})
    {
        exec 'mawk', '-Winteractive', $awkprogram;
    }
    else
    {
        exec 'mawk', $awkprogram;
    }

    exit; # dummy. We never get here
}

sub expr_col_names_to_indices
{
    # I take in a string with awk/perl code, and replace field references to
    # column references that the awk/perl program will understand
    #
    # The tricky thing here is that the field names could have come from a
    # transform expression, so I can have a field name 'a' AND another field
    # name 'rel(a)'. If I look for 'a' with a simple regex match of \ba\b then I
    # may erroneously match rel(a).
    #
    # Thus I look for each field one at a time, starting from the longest one,
    # and moving down the list.

    my ($language, $out) = @_;

    for my $key(reverse sort {length($a) <=> length($b)} keys %colindices_in)
    {
        # Pre- and post-field-name regexen. If the field name begins/ends with
        # an alphanumeric character (the usual case), this is the word boundary
        # \b. Otherwise an empty string. This is here so that field names such
        # as 'rel(a)' can match despite not ending in something that matches a
        # word boundary
        if( $language eq 'perl' )
        {
            my $post = $key =~ /\w$/ ? '\b' : '';
            my $re = qr/\$\Q$key\E$post/;
            $out =~ s/$re/\$feval[$colindices_in{$key}]/g;
        }
        elsif( $language eq 'awk' )
        {
            # column index that awk knows about
            my $colidx = $colindices_in{$key} + 1;
            my $pre  = $key =~ /^\w/ ? '\b' : '';
            my $post = $key =~ /\w$/ ? '\b' : '';
            my $re = qr/$pre\Q$key\E$post/; # \Q,\E to quote the ()
            $out =~ s/$re/\$$colidx/g;
        }
        else
        {
            die "Unknown language '$language";
        }
    }
    return $out;
}

sub get_expr_string
{
    sub slurp
    {
        my $filename = shift;
        my $fd;
        open $fd, '<', $filename or die "Couldn't open '$filename'";
        local $/ = undef;
        my $data = <$fd>;
        close $fd;
        return $data;
    }


    my $str = shift;
    return $str unless $str =~ /^\@(\S+)$/;


    return slurp($1);
}

my $must_match_expr =
  join ' && ',
  map { expr_col_names_to_indices( 'perl', $_) }
  map { get_expr_string($_) } @{$options{matches}};
$must_match_expr = 1 if !defined $must_match_expr || '' eq $must_match_expr;

my @f;
my $evalstr = 'sub matches { my @feval = map {q{-} eq $_ ? undef : $_ } @f; return ' . $must_match_expr . '; }';

if ( $options{eval} )
{
    my $expr = expr_col_names_to_indices( 'perl', get_expr_string($options{eval}) );
    $evalstr .= "\n" . 'sub evalexpr { my @feval = map {q{-} eq $_ ? undef : $_ } @f; ' . $expr . ' ; }';
}

if( $options{dumpexprs} )
{
    say "Expressions to evaluate:\n\n$evalstr";
    exit;
}

eval $evalstr;
if( $@ )
{
    die "Error evaluating expression '$evalstr':\n$@";
}




RECORD:
while(<STDIN>)
{
    # Data loop. Each statement here is analogous to the awk program generated
    # by makeAwkProgram();

    # skip comments
    if(/^#/)
    {
        unless($options{skipcomments})
        {
            print;
            flush STDOUT if $options{unbuffered};
        }
        next;
    }

    chomp;
    @f = split;

    # skip incomplete records. Can happen if a log line at the end of a file was
    # cut off in the middle
    next unless $indices_in_max <= $#f;

    # skip records that have empty input columns that must be non-empty
    next if any {'-' eq $f[$_]} @must_have_col_indices_in;

    # skip all records that don't match given expressions
    next unless matches();


    @f = @f[@indices_in];

    # skip empty records if we must
    next if $options{skipempty} && all {$_ eq '-' } @f;

    if( $options{eval} )
    {
        evalexpr();
    }
    else
    {
        # apply transformations
        for my $transform (@transforms)
        {
            my ($index_out, @funcs) = @$transform;
            foreach my $func (@funcs)
            {
                $f[$index_out] = $func->($f[$index_out]);
            }
        }
        print "@f\n";
        flush STDOUT if $options{unbuffered};
    }
}






sub parse_transform_funcs
{
    sub parse_transform_func
    {
        # Takes in a function name, and outputs a list
        #
        #     (perl function, awk function text to print, extra awk
        #     condition/command directives)
        my $f = shift;

        if( $f eq 'us2s' )
        {
            return
              ( sub { return $_[0] * 1e-6; },
                '(___input___) * 1e-6' );
        }
        elsif( $f eq 'deg2rad' )
        {
            return
              ( sub { return $_[0] * 3.141592653589793/180.0; },
                '(___input___) * 3.141592653589793/180.0' );
        }
        elsif( $f eq 'rad2deg' )
        {
            return
              ( sub { return $_[0] * 180.0/3.141592653589793; },
                '(___input___) * 180.0/3.141592653589793' );
        }
        elsif( $f eq 'rel' )
        {
            # relative to the starting value. The 'state' variable should be a
            # different instance for each sub instance
            return
              ( sub
                {
                    state $x0;
                    $x0 //= $_[0];
                    return $_[0] - $x0;
                },

                # statement to print
                '(___input___) - ___state___',

                # preceding awk stanza
                '!___state___set {___state___ = (___input___); ___state___set=1}'
              );
        }
        elsif( $f =~ /^rel_[ne]$/ )
        {
            # rel_n(lat) and rel_e(lon) take a lat/lon coord in degrees and
            # output a local N,E coord in meters, relative to the first lat/lon
            # value
            #
            # To a first approximation this is
            #   (N,E) = R_earth * (delta_lat, cos(lat0) * delta_lon)
            #
            # These are a bit special: here a single sub and a single set of
            # state is used for both rel_n and rel_e. And this state applies to
            # ALL rel_n, rel_e filters
            state ($lat0,$lon0,$cos_lat0);
            my $Rearth = 6371.0e3; # "mean radius" from wikipedia

            if ( $f eq 'rel_n')
            {
                $saw_rel_n = 1;


                return
                  ( sub
                    {
                        return '-' if '-' eq $_[0];

                        if ( !defined $lat0 )
                        {
                            $lat0 = $_[0];
                            $cos_lat0 = cos($lat0 * 3.141592653589793/180.0);
                            return 0;
                        }
                        return ($_[0] - $lat0) * 3.141592653589793/180.0 * $Rearth;
                    },

                    # these awk statements are a bit convoluted by nature. Write
                    # it out to confirm it is correct. The tests confirm it is
                    # correct.
                    #
                    # I don't use the special ___state___ and ___state___set
                    # variables because the state here is GLOBAL, not local to
                    # some particular column. I.e. lat0 is the same for ALL
                    # rel_n(). ___rel_[ne]0___ and ___cos_lat0___ are passed to
                    # awk unmodified

                    # statement to print
                    "(___input___) == \"-\" ? \"-\" : ((___input___) * 3.141592653589793/180.0*$Rearth - ___rel_n0___)",

                    # preceding awk stanza
                    "!___rel_n0___set && (___input___) != \"-\" {" .
                    "   x = (___input___) * 3.141592653589793/180.0;" .
                    "   ___rel_n0___ = x * $Rearth;"     .
                    "   ___cos_lat0___ = cos(x);"      .
                    "   cos_lat0_scaled = ___cos_lat0___ * 3.141592653589793/180.0 * $Rearth;" .
                    "   ___rel_n0___set=1" .
                    "}"
                  );
            }

            # 'rel_e'
            if( !$saw_rel_n )
            {
                # rel_n initializes state (cos(lat0)) that rel_e uses, so rel_n
                # must come first
                die "If using rel_n() and rel_e(), rel_n() MUST come first. Giving up";
            }


            return
              ( sub
                {
                    return '-' if '-' eq $_[0];
                    $lon0 //= $_[0];
                    return ($_[0] - $lon0) * 3.141592653589793/180.0 * $cos_lat0 * $Rearth;
                },

                # these awk statements are a bit convoluted by nature. Write
                # it out to confirm it is correct. The tests confirm it is
                # correct.
                #
                # I don't use the special ___state___ and ___state___set
                # variables because the state here is GLOBAL, not local to some
                # particular column. I.e. lat0 is the same for ALL rel_n().
                # ___rel_[ne]0___ and ___cos_lat0___ are passed to awk
                # unmodified

                # statement to print
                "(___input___) == \"-\" ? \"-\" : ((___input___) * cos_lat0_scaled - ___rel_e0___)",

                # preceding awk stanza
                "!___rel_e0___set && (___input___) != \"-\" {" .
                "   x = (___input___) * 3.141592653589793/180.0;" .
                "   ___rel_e0___ = x * $Rearth * ___cos_lat0___;"   .
                "   ___rel_e0___set=1" .
                "}"
              );
        }
        elsif( $f eq 'diff' )
        {
            # relative to the previous value. The 'state' variable should be a
            # different instance for each sub instance
            return
              ( sub
                {
                    state $xprev;
                    my $ret = 0;
                    if (defined $xprev)
                    {
                        $ret = $_[0] - $xprev;
                    }
                    $xprev = $_[0];
                    return $ret;
                },

                # statement to print
                '((___input___) - ___state___)*___state___set',

                # preceding awk stanza
                undef,

                # following awk stanza
                '{___state___ = (___input___); ___state___set=1}'
              );
        }
        else
        {
            die "Unknown transform function '$f'";
        }
    }


    my @func_names = @_;
    my @perl_funcs;
    my $awk_statement = '___input___';
    my ($awk_stanzas_before, $awk_stanzas_after);
    for my $f(@func_names)
    {
        # get the perl functions, the awk statement text, and any awk stanzas we
        # need
        my ($p, $a, $s_before, $s_after) = parse_transform_func($f);

        # the perl functions go into a list, to be applied one at a time during
        # data processing
        push @perl_funcs, $p;

        # the awk function composition happens right now when I generate the awk
        # statement string
        state $awk_state_idx = 0;
        my $state = "__state$awk_state_idx";
        $a =~ s/___input___/$awk_statement/g;
        $a =~ s/___state___/$state/g;

        if( defined $s_before )
        {
            $s_before =~ s/___input___/$awk_statement/g;
            $s_before =~ s/___state___/$state/g;
            $awk_stanzas_before  .= " $s_before ";
        }
        if( defined $s_after )
        {
            $s_after =~ s/___input___/$awk_statement/g;
            $s_after =~ s/___state___/$state/g;
            $awk_stanzas_after  .= " $s_after ";
        }

        $awk_statement = $a;
        $awk_state_idx++;
    }

    return (@perl_funcs, $awk_statement, $awk_stanzas_before, $awk_stanzas_after);
}

sub makeAwkProgram
{
    # The awk program I generate here is analogous to the logic in the data
    # while() loop above
    my $awkprogram = '';

    # skip comments
    $awkprogram .=
      '/^#/ { ' . ($options{skipcomments} ? '' : 'print; ') . 'next } ';

    # skip incomplete records. Can happen if a log line at the end of a file
    # was cut off in the middle
    $awkprogram .=
      (1+$indices_in_max) . " > NF { next } ";

    # skip records that have empty input columns that must be non-empty
    if (@must_have_col_indices_in)
    {
        $awkprogram .=
          join(' || ', map { '$'.($_+1). " == \"-\"" } @must_have_col_indices_in);
        $awkprogram .= ' { next } ';
    }

    for my $expr( @{$options{matches}} )
    {
        $expr = expr_col_names_to_indices('awk', get_expr_string($expr));
        $awkprogram .= ' !' . "($expr) { next } ";
    }

    # skip empty records if we must
    if ($options{skipempty})
    {
        $awkprogram .=
          join ' && ', (map { '$'.($_+1)." == \"-\"" } @indices_in);
        $awkprogram .= ' { next } ';
    }

    if( $options{eval} )
    {
        my $expr = expr_col_names_to_indices('awk', get_expr_string($options{eval}));
        $awkprogram .= "$expr";

        # don't need $awk_state_stanzas_before and $awk_state_stanzas_after:
        # these are for rel() and diff(), and I'm disallowing column selectors
    }
    else
    {
        # print selected fields. These already have the transformations applied
        $awkprogram .= $awk_state_stanzas_before if defined $awk_state_stanzas_before;
        $awkprogram .=
          '{ print ' . (join(',', @awk_output_fields)) . '} ';
        $awkprogram .= $awk_state_stanzas_after if defined $awk_state_stanzas_after;
    }

    return $awkprogram;
}

__END__

=head1 NOTICE

This is undergoing a rework. Don't read (and believe) the documentation just
yet.


=head1 NAME

asciilog-filter - filters ascii logs to select particular rows, fields

=head1 SYNOPSIS

    # Read log data, filter out the timestamps, and post-process some of them
    $ < /tmp/log
      asciilog-filter '.*time' 'us2s(time)' 'rel(time)' 'rel(us2s(.*time))'

    # image_time time us2s(time) rel(time) rel(us2s(image_time)) rel(us2s(time))
    1434662133270338 1434662131279978 1434662131.27998 0 0 0
    1434662133270338 1434662131289978 1434662131.28998 10000 0 0.00999999046325684
    1434662133270338 1434662131299978 1434662131.29998 20000 0 0.0199999809265137
    1434662133270338 1434662131309978 1434662131.30998 30000 0 0.0299999713897705
    ...

    # Run the robot, write out telemetry to a log, filter out the position, and
    # make a realtime plot of its motion
    $ run_robot |
      tee /tmp/log |
      asciilog-filter --unbuffered x y |
      feedgnuplot --domain --lines --stream

    [ plot pops up, and updates itself as the robot moves ]


=head1 DESCRIPTION

This tool reads in an ASCII data stream, and allows easy filtering to select
particular data from this stream. Many common post-processing operations are
available, as is general interpretation of data.

This is a UNIX-style tool, so the input/output of this tool is strictly
STDIN/STDOUT. Furthermore, in its most common form this tool is a filter, so the
format of the output is I<exactly> the same as the format of the input. The
exception to this is when using C<--eval>, in which the output is dependent on
whatever expression we're evaluating.

This tool is convenient both to process stored data, or to process live data
that can then be plotted to produce realtime telemetry.

This tool takes a list of fields on the commandline. These are the only fields
that are selected for output. The requested field names are compared with the
fields listed in the legend of the data. If an exact match is found, we select
that column. Otherwise we run a regex search, and take all matching columns.

The user often wants to apply unit conversions to data, or to look at the data
relative to the initial point, or to differentiate the input. These filters can
be easily applied by this tool.

This tool has two major modes of operation: filtering and evaluating.

The normal mode of operation is a filtering one. Here we read in an asciilog,
select some rows, columns, maybe apply some simple filter functions, and output
this data as a new asciilog. A new legend is written, and comments are
preserved.

If we want to do something more complicated, we can use the I<evaluating> mode
of operation. Here we pass C<--eval expression>. For each row, we evaluate the
expression (in awk, or in perl if C<--perl> is given). This evaluation may or
may not print anything. Comments are ignored. Any columns should be referred-to
by name (not as a regex or a transform). This mode ignores any column selectors,
so none should be given.

=head2 Input/output data format

The input/output data is simply an ASCII table of values. Any lines beginning
with C<##> are treated as comments, and are passed through. The first line that
begins with C<#> but not C<##> is a I<legend> line. After the C<#>, follow
whitespace-separated ASCII field names. Each subsequent line is
whitespace-separated values matching this legend. For instance, this is a valid
data file:

    ## log version: 3 ins_type: RAW_LOG_INS_440 camera_type: Unknown camera_type id: 5
    ## camera 0: serial 0,1 cols/rows: 3904 3904 channels: 1 depth: 8
    ## camera 1: serial 2,3 cols/rows: 3904 3904 channels: 1 depth: 8
    ## camera 2: serial 4,0 cols/rows: 3904 3904 channels: 1 depth: 8
    ## camera 3: serial 0,0 cols/rows: 0 0 channels: 0 depth: 0
    # x_rate y_rate z_rate
    -0.016107 0.004362 0.005369
    -0.017449 0.006711 0.006711
    -0.018456 0.014093 0.006711
    -0.017449 0.018791 0.006376

This is the format for both the input and the output. This tool makes sure to
update the legend to reflect which columns have been selected.

A string C<-> is used to indicate that a record does not have a value for this
field. Other records (lines) may have such a value.

=head2 Basic filtering

To select specific columns from a file, pass their names (or regexen) on the
commandline. For instance to pull out columns called C<lat>, C<lon>, and any
column whose name contains the string C<feature_>, do

 asciilog-filter lat lon feature_

We look for exact column name matches, and if none are found, we use a regex. So
the above would work if we had columns C<feature_width> and C<feature_height>,
but would fail if we also had C<feature_>, since in this case only this last
column would be found. We can explicitly pass a regex to deal with this:

 asciilog-filter lat lon 'feature_.*'

To select specific rows, we can use C<--has col> to get only those rows that
have a non-empty value in column C<col>. C<--matches> is similar, but evaluates
arebitrary expressions. For instance C<--matches 'size E<gt> 10'> would select
only those rows whose C<size> column contains a value > 10. Note that
C<--matches> can I<only> see data currently in the input asciilog, and can
I<not> apply L<transform expressions|Transforms>. So if you want C<--matches
rel(x)E<gt>5> then you must first invoke C<asciilog-filter> to produce this
column. I.e. this doesn't work:

 <data asciilog-filter --matches 'rel(x)>5'

But this /does/ work:

 <data asciilog-filter . 'rel(x)' | asciilog-filter --matches 'rel(x)>5'

If we want to select a column I<and> pick only rows that have a value in this
column, a shorthand syntax exists:

 asciilog-filter +col

is equivalent to

 asciilog-filter --has col col

=head2 Transforms

We can post-process our data with some simple transforms. To apply a transform
=f= and then a transform =g= to column =x=, pass in =g(f(x))=, as expected. Note
that the resulting legend contains fields with =()= in their names, but these
remain filter-able with further =asciilog-filter= invocations, and C<--matches>
and C<--eval> remain functional. The only weirdness is that if we're using perl
you need to say C<--matches $rel(x)E<gt>5> instead of C<--matches rel($x)E<gt>5>.

The transforms currently available are

=over

=item C<us2s>

convert microseconds to seconds

=item C<deg2rad>

convert degrees to radians

=item C<rad2deg>

convert radians to degrees

=item C<rel>

report data relative to first value

=item C<rel_n>, C<rel_e>

Can be applied to latitude and longitude (in degrees) respectively to report
coordinates in a local coordinates system aligned with North and East relative
to the first latlon, in meters. If using these, you I<must> have one of each and
C<rel_n> I<must> come first. Generally you'll filter thusly:

    'rel_n(lat)' 'rel_e(lon)'

=item C<diff>

report data relative to previous value

=back

=head2 Backend choice

By default, the parsing of arguments and the legend happens in perl, which then
constructs a simple awk script, and invokes C<awk> to actually read the data and
to process it. This is done because awk is lighter weight and runs faster, which
is important because our data sets could be quite large. This is especially true
of C<mawk>, which is noticeably more snappy than C<gawk>. We don't need the
extra features of C<gawk>, so C<mawk> is preferred here. If for whatever reason
we want to do everything with perl, this can be requested with the C<--perl>
option.

=head1 ARGUMENTS

=head2 --has a,b,c,...

Used to select particular records (rows) in a data file. A I<null> value in a
column is designated with a single C<->. This means that this particular field
value does not exist in this record. If we want to select only records that
I<do> have a column named C<x>, we can pass C<--has x>. To select records that
have data for I<all> of a set of columns, the C<--has> option can be repeated,
or these multiple columns can be given in a whitespace-less comma-separated
list. For instance if we want only records that have data in I<both> columns
C<x> I<and> C<y> we can pass in C<--has x,y> or C<--has x --has y>. If we want
to combine multiple columns in an I<or> (select rows that have data in I<any> of
a given set of columns), use C<--matches> as documented below.

If we want to select a column I<and> pick only rows that have a value in this
column, a shorthand syntax exists:

 asciilog-filter --has col col

is equivalent to

 asciilog-filter +col

=head2 --matches expr

Used to select particular records (rows) in a data file. The argument is an
expression that is evaluated for each row. If it evaluates to true, that row is
output. This expression is passed directly to the perl or awk backend, so be
aware of which one you are using. This feature is slow in perl, so awk is
strongly recommended here. To refer to a field named C<xxx> in the expression,
say C<xxx> in awk and C<$xxx> in perl. There's a small caveat here: C<xxx> is a
field-name, even if it contains non-alphanumerics, so if you want to pull out
only those records where C<rel(x) E<gt> 5>, then in perl you say this:

 <data asciilog-filter . 'rel(x)' | asciilog-filter --perl --matches '$rel(x) > 5'

Note the C<$rel(x)> and /not/ C<rel($x)>.

Multiple C<--matches> options can be given. The record is output only if I<all>
the expressions are true.

If a C<--matches> expression begins with C<@>, then the text following the C<@>
is assumed to be a filename from which the expression should be read.

Example: to select all rows that have valid data in column C<a> I<or> column
C<b> I<or> column C<c> you can

 asciilog-filter --matches 'a != "-" || b != "-" || c != "-"'

or

 asciilog-filter --perl --matches 'defined $a || defined $b || defined $c'

=head2 --eval expr

Instead of printing out all matching records, evaluate the expression for each
record. The expression may or may not print anything. The expression is treated
similar to --matches: variables are bound by name, a sigil may be required,
depending on language and so on. No column-selecting arguments should be given.

If a C<--eval> expression begins with C<@>, then the text following the C<@> is
assumed to be a filename from which the expression should be read.

In perl the arbitrary expression fits in like this:

 while(<>) # read each line
 {
   next unless matches; # skip non-matching lines
   eval expression;     # evaluate the arbitrary expression
 }

In awk the expr is a full set of pattern/action statements. So to print the sum
of C<a> and C<b>:

 asciilog-filter --eval '{print a+b}'

To print the sum of some column:

 asciilog-filter --eval '{sum += x} END {print sum}'

=head2 --[no]skipempty

Do [not] skip records where all fields are blank. By default we I<do> skip all
empty records; to include them, pass C<--noskipempty>

=head2 --skipcomments

Don't output non-legend comments

=head2 --perl

Normally we initialize with perl, and use awk to do all the actual work. If we
want to do everything with perl, pass this option. It should produce the same
results, but not as quickly.

=head2 --dumpexprs

Used for debugging. This spits out all the expressions parsed from the
commandline. These come from C<--matches>, C<--eval> and, if using the awk
backend, the generated awk program

=head2 --dumpindices

Used for debugging. If given, prints out the indices of all the selected
columns, and exits.

=head2 --unbuffered

Flushes each line after each print. This makes sure each line is output as soon
as it is available, which is crucial for realtime output and plotting.

=head1 REPOSITORY

https://github.com/dkogan/asciilog/

=head1 AUTHOR

Dima Kogan C<< <dima@secretsauce.net> >>

=head1 LICENSE AND COPYRIGHT

Copyright 2016 California Institute of Technology
Copyright 2017 Dima Kogan


This library is free software; you can redistribute it and/or modify it under
the terms of the GNU Lesser General Public License as published by the Free
Software Foundation; either version 2.1 of the License, or (at your option) any
later version.

=cut
